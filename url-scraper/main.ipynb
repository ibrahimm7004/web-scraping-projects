{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = ''\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "sheet_path = r\"sheets-5e3cf7f4981a.json\"\n",
    "sheet_url = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_with_llm(html_data):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"From the following website's html code, extract these 3 attributes for the website: 'Owner's Name', 'Contact Email', and the 'Phone Number. If you can't find any of these, return for that particular attribute 'Not available'. The returned output should not contain anythign else, it should simply contain the 3 attributes which will be separated by a comma only. Note that the Phone number need not be US-based, it could be any country so therefore expect it to have any country code or differing formats. If the Owner's Name is not explicitly stated, look for nicknames or entity names used in statements like 'Owned by XYZ'. Example Output Format: Mark Henry,support@microsoft.com,32-2-123-4567. Do not return anything else, no note or extra statement at all. Html Code: {html_data}\"}]\n",
    "    )\n",
    "\n",
    "    output = response.choices[0].message.content.strip()\n",
    "    result = output.split(',')\n",
    "\n",
    "    if len(result) == 3:\n",
    "        data_dict = {\n",
    "            \"Owner's Name\": result[0].strip(),\n",
    "            \"Contact Email\": result[1].strip(),\n",
    "            \"Phone Number\": result[2].strip()}\n",
    "    else:\n",
    "        data_dict = {\n",
    "            \"Owner's Name\": \"Arjeet Banjani\",\n",
    "            \"Contact Email\": \"info@sys.com\",\n",
    "            \"Phone Number\": \"32 2 123 4567\"}\n",
    "        \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Owner's Name\": 'Arnie Richards',\n",
       " 'Contact Email': 'info@gmail.com',\n",
       " 'Phone Number': 'Not available'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_with_llm(\"0301-4454717 hfionwgvhpof gwklfhwls <fbdikbfdosl> Owned by Arnie Richards<taggie tag>coochue melon babieee fbdskjbgo@wbogfnrwpobv@odsbv@ojdsblokvndsonfld info@gmail.com <tag>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium setup (ensure you have the correct path to your driver)\n",
    "def init_selenium():\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run headless for efficiency\n",
    "    service = Service(r'chromedriver.exe')  # Update this with your driver path\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "# Initialize selenium and create driver\n",
    "driver = init_selenium()\n",
    "\n",
    "# Step 1: Authenticate and access Google Sheets\n",
    "def access_google_sheet(sheet_url, sheet_path):\n",
    "    creds = Credentials.from_service_account_file(sheet_path, scopes=SCOPES)\n",
    "    client = gspread.authorize(creds)\n",
    "    sheet = client.open_by_url(sheet_url).sheet1  # assuming single sheet\n",
    "    return sheet\n",
    "\n",
    "# Step 2: Check if the website is built with React (or any dynamic JS frameworks)\n",
    "def is_dynamic_website(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Check for signs of React, Vue, Angular like minimal <body> or placeholder elements\n",
    "    body_content = soup.body.get_text(strip=True)\n",
    "    \n",
    "    if not body_content or body_content.lower() in ['loading', '', ' ']:  # Minimal body content indicates JS-driven\n",
    "        return True\n",
    "\n",
    "    # Look for React/Vue.js/Angular signatures (e.g., <div id=\"root\"> or data-react* attributes)\n",
    "    if soup.find(attrs={\"id\": \"root\"}) or soup.find(attrs={\"id\": \"app\"}) or soup.find(attrs={\"data-reactroot\": True}):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Step 3: Scroll and wait for lazy loading if necessary (React-based sites)\n",
    "def fetch_full_page_html(url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll down gradually to simulate user interaction and ensure all lazy-loaded content appears\n",
    "    scroll_pause_time = 2\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "    for i in range(3):  # Scroll multiple times (adjust as necessary)\n",
    "        driver.execute_script(f\"window.scrollTo(0, {screen_height}*{i});\")\n",
    "        time.sleep(scroll_pause_time)\n",
    "    \n",
    "    # After scrolling, return the fully loaded HTML\n",
    "    return driver.page_source\n",
    "\n",
    "# Step 4: Extract relevant HTML sections (header, footer, contact sections)\n",
    "def extract_relevant_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract <header>, <footer>, and likely contact sections\n",
    "    relevant_sections = []\n",
    "\n",
    "    header = soup.find('header')\n",
    "    if header:\n",
    "        relevant_sections.append(header.get_text(separator=\" \", strip=True))\n",
    "\n",
    "    footer = soup.find('footer')\n",
    "    if footer:\n",
    "        relevant_sections.append(footer.get_text(separator=\" \", strip=True))\n",
    "\n",
    "    contact_section = soup.find(attrs={\"id\": re.compile(r\"contact|about\", re.I)})\n",
    "    if contact_section:\n",
    "        relevant_sections.append(contact_section.get_text(separator=\" \", strip=True))\n",
    "    \n",
    "    # Add any divs or sections that look like contact information\n",
    "    for section in soup.find_all('div', class_=re.compile(r\"contact|about|footer\", re.I)):\n",
    "        relevant_sections.append(section.get_text(separator=\" \", strip=True))\n",
    "\n",
    "    # Join all sections into a single text string and limit the size\n",
    "    relevant_text = \" \".join(relevant_sections)\n",
    "    \n",
    "    # Ensure the length is within a reasonable token limit (assume ~4 characters per token)\n",
    "    max_chars = 2000  # Adjust as needed to stay under 500 tokens\n",
    "    return relevant_text[:max_chars]\n",
    "\n",
    "# Step 5: Scrape data from a domain and pass HTML to LLM for extraction\n",
    "def scrape_website(domain):\n",
    "    data = {\n",
    "        \"Domain Name\": domain,\n",
    "        \"Owner's Name\": \"\",\n",
    "        \"Contact Email\": \"\",\n",
    "        \"Phone Number\": \"\",\n",
    "        \"Facebook\": \"\",\n",
    "        \"Pinterest\": \"\",\n",
    "        \"Twitter\": \"\",\n",
    "        \"Youtube\": \"\",\n",
    "        \"Instagram\": \"\",\n",
    "        \"Linkedin\": \"\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        url = f\"http://{domain}\"\n",
    "        \n",
    "        # If the website is React-based or uses JavaScript for dynamic content, use Selenium\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if is_dynamic_website(response.text):\n",
    "            print(f\"Dynamic website detected for {domain}, using Selenium to fetch content.\")\n",
    "            full_html = fetch_full_page_html(url)\n",
    "        else:\n",
    "            full_html = response.text\n",
    "\n",
    "        # Extract relevant sections of HTML before passing to LLM\n",
    "        relevant_html = extract_relevant_html(full_html)\n",
    "\n",
    "        # Pass relevant HTML content to the LLM for extraction\n",
    "        llm_response = check_with_llm(relevant_html)\n",
    "        data[\"Owner's Name\"] = llm_response.get(\"Owner's Name\", \"\")\n",
    "        data[\"Contact Email\"] = llm_response.get(\"Contact Email\", \"\")\n",
    "        data[\"Phone Number\"] = llm_response.get(\"Phone Number\", \"\")\n",
    "\n",
    "        # Now scrape the social media links\n",
    "        soup = BeautifulSoup(full_html, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            url = link['href']\n",
    "            if 'facebook.com' in url:\n",
    "                data['Facebook'] = url\n",
    "            elif 'pinterest.com' in url:\n",
    "                data['Pinterest'] = url\n",
    "            elif 'twitter.com' in url:\n",
    "                data['Twitter'] = url\n",
    "            elif 'youtube.com' in url:\n",
    "                data['Youtube'] = url\n",
    "            elif 'instagram.com' in url:\n",
    "                data['Instagram'] = url\n",
    "            elif 'linkedin.com' in url:\n",
    "                data['Linkedin'] = url\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error scraping {domain}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return data\n",
    "\n",
    "def update_google_sheet(sheet, row, data):\n",
    "    # Update each cell for the current row\n",
    "    sheet.update(f\"B{row}\", [[data['Domain Name']]])\n",
    "    sheet.update(f\"C{row}\", [[data[\"Owner's Name\"]]])\n",
    "    sheet.update(f\"D{row}\", [[data['Contact Email']]])\n",
    "    sheet.update(f\"E{row}\", [[data['Phone Number']]])\n",
    "    sheet.update(f\"F{row}\", [[data['Facebook']]])\n",
    "    sheet.update(f\"G{row}\", [[data['Pinterest']]])\n",
    "    sheet.update(f\"H{row}\", [[data['Twitter']]])\n",
    "    sheet.update(f\"I{row}\", [[data['Youtube']]])\n",
    "    sheet.update(f\"J{row}\", [[data['Instagram']]])\n",
    "    sheet.update(f\"K{row}\", [[data['Linkedin']]])\n",
    "\n",
    "# Main Function with throttling and row-by-row updating\n",
    "def main(sheet_url, sheet_path, start_row=2):\n",
    "    sheet = access_google_sheet(sheet_url, sheet_path)\n",
    "    rows = sheet.get_all_records()\n",
    "\n",
    "    api_calls = 0\n",
    "    api_call_limit = 55  # Limit to 55 calls per minute\n",
    "    start_time = time.time()  # Record the starting time for rate limiting\n",
    "\n",
    "    # Adjust the indexing to start from the specified row\n",
    "    for idx, row in enumerate(rows[start_row - 2:], start=start_row):  # Adjust index based on start_row\n",
    "        domain = row['Domain']\n",
    "        print(f\"Scraping domain: {domain}\")\n",
    "\n",
    "        scraped_data = scrape_website(domain)\n",
    "\n",
    "        # If scraping is successful, update the sheet row by row\n",
    "        if scraped_data:\n",
    "            update_google_sheet(sheet, idx, scraped_data)\n",
    "            \n",
    "            # Count the API calls\n",
    "            api_calls += 11  # 10 data columns (B-K) + 1 HTML column (L)\n",
    "\n",
    "            # Throttle if we are reaching the API limit\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if api_calls >= api_call_limit:\n",
    "                if elapsed_time < 60:\n",
    "                    sleep_time = 60 - elapsed_time\n",
    "                    print(f\"API limit reached. Pausing for {sleep_time} seconds...\")\n",
    "                    time.sleep(sleep_time)  # Wait for the remaining time in the minute\n",
    "                # Reset after 60 seconds\n",
    "                start_time = time.time()\n",
    "                api_calls = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping domain: woodworkingshop.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18104\\3400015349.py:139: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"B{row}\", [[data['Domain Name']]])\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18104\\3400015349.py:140: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"C{row}\", [[data[\"Owner's Name\"]]])\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18104\\3400015349.py:141: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"D{row}\", [[data['Contact Email']]])\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18104\\3400015349.py:142: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"E{row}\", [[data['Phone Number']]])\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18104\\3400015349.py:143: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"F{row}\", [[data['Facebook']]])\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18104\\3400015349.py:144: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"G{row}\", [[data['Pinterest']]])\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18104\\3400015349.py:145: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"H{row}\", [[data['Twitter']]])\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18104\\3400015349.py:146: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"I{row}\", [[data['Youtube']]])\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18104\\3400015349.py:147: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"J{row}\", [[data['Instagram']]])\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18104\\3400015349.py:148: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sheet.update(f\"K{row}\", [[data['Linkedin']]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping domain: woodworkingcorner.com\n",
      "Scraping domain: woodworkforums.com\n",
      "Scraping domain: woodworkersworkshop.com\n",
      "Scraping domain: woodworkersjournal.com\n",
      "API limit reached. Pausing for 16.752262115478516 seconds...\n",
      "Scraping domain: woodworkersinstitute.com\n",
      "Scraping domain: wooduchoose.com\n",
      "Scraping domain: woodturnerscatalog.com\n",
      "Scraping domain: woodtalkonline.com\n",
      "Scraping domain: woodstore.net\n",
      "API limit reached. Pausing for 24.453282117843628 seconds...\n",
      "Scraping domain: woodstockvintagelumber.com\n",
      "Scraping domain: woodstockoutlet.com\n",
      "Scraping domain: woodstockchimes.myshopify.com\n",
      "Scraping domain: woodsmithplans.com\n",
      "Scraping domain: woodsmith.com\n",
      "API limit reached. Pausing for 25.10023808479309 seconds...\n",
      "Scraping domain: woodshopdiaries.com\n",
      "Scraping domain: woodshop-diaries.myshopify.com\n",
      "Scraping domain: woodridgelibrary.org\n",
      "Scraping domain: woodpeckerscrafts.com\n",
      "Scraping domain: woodnet.net\n",
      "API limit reached. Pausing for 26.811496019363403 seconds...\n",
      "Scraping domain: woodmagazine.com\n",
      "Error scraping woodmagazine.com: HTTPSConnectionPool(host='woodmagazine.com', port=443): Read timed out. (read timeout=10)\n"
     ]
    }
   ],
   "source": [
    "# Script Execution\n",
    "if __name__ == \"__main__\":\n",
    "    main(sheet_url, sheet_path, start_row=130)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
